<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Linear temp</title>

<script src="site_libs/header-attrs-2.25/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; } /* Alert */
code span.an { color: #008000; } /* Annotation */
code span.at { } /* Attribute */
code span.bu { } /* BuiltIn */
code span.cf { color: #0000ff; } /* ControlFlow */
code span.ch { color: #008080; } /* Char */
code span.cn { color: #585cf6; } /* Constant */
code span.co { color: #4c886b; } /* Comment */
code span.cv { color: #008000; } /* CommentVar */
code span.do { color: #008000; } /* Documentation */
code span.dv { color: #0000cd; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cd; } /* Float */
code span.im { } /* Import */
code span.in { color: #008000; } /* Information */
code span.kw { color: #0000ff; } /* Keyword */
code span.op { color: #687687; } /* Operator */
code span.ot { color: #ff4000; } /* Other */
code span.pp { color: #ff4000; } /* Preprocessor */
code span.sc { color: #008080; } /* SpecialChar */
code span.ss { color: #008080; } /* SpecialString */
code span.st { color: #036a07; } /* String */
code span.va { } /* Variable */
code span.vs { color: #008080; } /* VerbatimString */
code span.wa { color: #008000; font-weight: bold; } /* Warning */

.sourceCode .row {
  width: 100%;
}
.sourceCode {
  overflow-x: auto;
}
.code-folding-btn {
  margin-right: -30px;
}
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">OCEAN_5098</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="environment.html">Environment</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Manipulation
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="basics.html">Basics</a>
    </li>
    <li>
      <a href="dplyr.html">dplyr</a>
    </li>
    <li>
      <a href="tidyr.html">tidyr</a>
    </li>
  </ul>
</li>
<li>
  <a href="types.html">Types &amp; structures</a>
</li>
<li>
  <a href="graphs.html">Graphics</a>
</li>
<li>
  <a href="maps.html">Mapping</a>
</li>
<li>
  <a href="functions.html">Loop &amp; functions</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="stat_1.html">Initiation</a>
    </li>
    <li>
      <a href="stat_2.html">Linear models</a>
    </li>
    <li>
      <a href="stat_3.html">Multivariates</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Linear temp</h1>

</div>


<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(gvlma)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span></code></pre></div>
<div id="linear-regression" class="section level1">
<h1>Linear regression</h1>
<div id="one-predictor" class="section level2">
<h2>One predictor</h2>
<p>Regression is somehow different from correlation because it try to
put variables into equation and thus explain relationship between them.
In <strong>Model I regression</strong> (indeed we have Model II when two
variables in the regression equation are random , i.e. no controlled by
the researcher), it is used to predict a quantitative outcome of a
dependent variable <span class="math inline">\(y\)</span> on the basis
of one single independent predictor variable <span
class="math inline">\(x\)</span>. The goal is to build a mathematical
model (or formula) that defines <span class="math inline">\(y\)</span>
as a function of the <span class="math inline">\(x\)</span> variable,
and that’s why why linear regression are also called linear modelling
(previously added to our scatter plot using <code>abline</code> or
<code>geom_smooth</code>.</p>
<p>Note that the <code>linear</code> in linear model does not imply a
straight-line relationship but rather that the response is a linear
(additive) combination of the effects of the explanatory variables.
However, because we tend to start by fitting the simplest relationship,
many linear models are represented by straight lines.</p>
<p>Once, we built a statistically significant model, it’s possible to
use it for predicting future outcome on the basis of new <span
class="math inline">\(x\)</span> values.</p>
<div id="formula-and-basics" class="section level3">
<h3>Formula and basics</h3>
<p>The formula of linear regression can be written as follows: <span
class="math display">\[ y = \beta_0 + \beta_1*x + \epsilon \]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span> are known as the regression
<strong>beta coefficients</strong> or <strong>parameters</strong>:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the
<strong>intercept</strong> of the regression line; that is the predicted
value when <em>x = 0</em>.</li>
<li><span class="math inline">\(\beta1\)</span> is the
<strong>slope</strong> of the regression line.</li>
</ul></li>
<li><p><span class="math inline">\(\epsilon\)</span> is the
<strong>error term</strong> (also known as the <strong>residual
errors</strong>).The error term is drawn from a statistical distribution
that captures the random variability in the response. In standard linear
regression this is assumed to be a normal (Gaussian)
distribution.</p></li>
</ul>
<p>The figure below illustrates the linear regression model, where:</p>
<ul>
<li>the best-fit regression line is in blue</li>
<li>the intercept (<span class="math inline">\(\beta_0\)</span>) and the
slope (<span class="math inline">\(\beta_1\)</span>) are shown in
green</li>
<li>the error terms (<span class="math inline">\(\epsilon\)</span>) are
represented by vertical red lines</li>
</ul>
<p><img src="illustrations/linear-regression.png" width="50%" style="display: block; margin: auto;" /></p>
<p>From the figure above, it can be seen that not all the data points
fall exactly on the fitted regression line. Some of the points are above
the blue curve and some are below it; overall, the residual errors
(<span class="math inline">\(\epsilon\)</span>) have approximately mean
zero.</p>
<p><strong>TERMINOLOGY ALERT</strong></p>
<p>The sum of the squares of the residual errors are called the
<strong>Residual Sum of Squares</strong> or <strong>RSS</strong>.</p>
<p>The average variation of points around the fitted regression line is
called the <strong>Residual Standard Error (RSE)</strong>. This is one
the metrics used to evaluate the overall quality of the fitted
regression model. The lower the RSE, the better it is.</p>
<p>Since the mean error term is zero, the outcome variable y can be
approximately estimated as follow:</p>
<p><span class="math display">\[y= \beta_0+\beta_1*x\]</span></p>
<p>Mathematically, the beta coefficients (<span
class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>) are determined so that the
<strong>RSS</strong> is as minimal as possible. This method of
determining the beta coefficients is technically called <strong>least
squares</strong> regression or <strong>ordinary least squares</strong>
(OLS) regression.</p>
<p>Once, the beta coefficients are calculated, a <em>t</em>-test is
performed to check whether or not these coefficients are significantly
different from zero. A non-zero beta coefficients means that there is a
significant relationship between the predictors (<span
class="math inline">\(x\)</span>) and the outcome variable (<span
class="math inline">\(y\)</span>).</p>
</div>
<div id="running-the-analysis" class="section level3">
<h3>Running the analysis</h3>
<p>So, the goal in linear regression is obtain the best estimates for
the model coefficients (<span class="math inline">\(\alpha\)</span> and
<span class="math inline">\(\beta\)</span>). In R you can fit linear
models using the function <code>lm</code>.</p>
<p>In this example, we will use a data set on plant heights around the
world <code>Plant_height.csv</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>Plant_height <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;Data/Plant_height.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>The main argument to <code>lm</code> is the model formula
<code>y ~ x</code>, where the response variable is on the left of the
tilde symbol (<code>~</code>) and the explanatory variable is on the
right. <code>lm</code> also has an optional data argument that lets you
specify a data frame from which the variables will be taken.</p>
<p>To test whether plant height is associated with temperature , we
would model height as the dependent variable (in this case we are using
the log of plant height) and temperature as the predictor variable <span
class="math inline">\(loght = \beta_0 + \beta_1 * temp\)</span>.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>mod1_plant<span class="ot">&lt;-</span> <span class="fu">lm</span>(loght <span class="sc">~</span> temp, <span class="at">data =</span> Plant_height)</span></code></pre></div>
<p>We can extracted the beta coefficients of this linear model
using:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>mod1_plant<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)        temp 
## -0.22566451  0.04241362</code></pre>
<p>The estimates for the coefficients give you the slope(<span
class="math inline">\(\beta_0\)</span>) and intercept (<span
class="math inline">\(\beta_1\)</span>) for the <code>temp</code>
variable. In this example, the regression equation for (log) plant
height as a function of temperature is:</p>
<p><span class="math inline">\(log(plantheight) = -0.22566 + 0.04241 *
temp + \epsilon\)</span></p>
<ul>
<li><p>the intercept (<span class="math inline">\(\beta_0\)</span>) is
<span class="math inline">\(-0.22566\)</span>. It can be interpreted as
the predicted (log) plant heigth when temperature is equal to zero.
Regression through the origin is when you force the intercept of a
regression model to equal zero. It’s also known as fitting a model
without an intercept (e.g., the intercept-free linear model <span
class="math inline">\(y = \beta_1*x\)</span> is equivalent to the model
<span class="math inline">\(y = \beta_0 + \beta_1*x\)</span> with <span
class="math inline">\(\beta_0=0\)</span>). Knowing that the <strong>true
relationship</strong> between your predictors and the expected value of
your dependent variable has to pass through the origin would be a good
reason for forcing the estimated relationship through the origin if you
knew for certain what the true relationship was (be careful very rare
cases where it is justified to remove the intercept).</p></li>
<li><p>the regression beta coefficient for the variable
<code>temp</code> (<span class="math inline">\(\beta1\)</span>), also
known as the slope, is <span class="math inline">\(0.04241\)</span>.
This means that, for one unit of <span
class="math inline">\(temp\)</span>, we can expect an increase of <span
class="math inline">\(0.04241\)</span> units in <span
class="math inline">\(log(plantheight)\)</span>.</p></li>
</ul>
<p>Calling <code>summary</code> on a model object produces a lot of
useful information but one of the main things to look out for are the
t-statistics and p-values for each coefficient. These test the null
hypothesis that the true value for the coefficient is 0.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">summary</span>(mod1_plant)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght ~ temp, data = Plant_height)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.97903 -0.42804 -0.00918  0.43200  1.79893 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.225665   0.103776  -2.175    0.031 *  
## temp         0.042414   0.005593   7.583 1.87e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6848 on 176 degrees of freedom
## Multiple R-squared:  0.2463, Adjusted R-squared:  0.242 
## F-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12</code></pre>
<p>Looking at only the p-values, this simple model seems to fit the data
very well. For the intercept we usually don’t care if it is zero or not,
but for the other coefficient (the slope), a value significantly
differing from zero indicates that there is an association between that
explanatory variable and the response. In this example, an increase in
temperature is associated with an increase in plant height. But
<code>lm</code> output tells us much more. The summary outputs shows 6
components, including</p>
<ul>
<li><p><strong>Call</strong> shows the function call used to compute the
regression model.</p></li>
<li><p><strong>Residuals</strong> provide a quick view of the
distribution of the residuals, which by definition have a mean zero.
Therefore, the median should not be far from zero, and the minimum and
maximum should be roughly equal in absolute value.</p></li>
<li><p><strong>Coefficients</strong> shows the regression beta
coefficients and their statistical significance. Predictor variables,
that are significantly associated to the outcome variable, are marked by
stars.</p></li>
<li><p><strong>Residual standard error</strong> (RSE),
<strong>R-squared</strong> (R2) and the <strong>F-statistic</strong> are
metrics that are used to check how well the model fits to our
data.</p></li>
</ul>
<div id="coefficients-significance" class="section level4">
<h4>Coefficients significance</h4>
<p>The coefficients table, in the model statistical summary, shows:</p>
<ul>
<li><p>the estimates of the <strong>beta coefficients</strong></p></li>
<li><p>the <strong>standard errors</strong> (SE), which defines the
accuracy of beta coefficients. For a given beta coefficient, the SE
reflects how the coefficient varies under repeated sampling. It can be
used to compute the confidence intervals and the t-statistic.</p></li>
<li><p>the <strong><em>t</em>-statistic</strong> and the associated
<strong><em>p</em>-value</strong>, which defines the statistical
significance of the beta coefficients.</p></li>
</ul>
<p><strong>1 - t-statistic and p-values</strong></p>
<p>For a given predictor, the t-statistic (and its associated
<em>p</em>-value) tests whether or not there is a statistically
significant relationship between a given predictor and the outcome
variable, that is whether or not the beta coefficient of the predictor
is significantly different from zero.</p>
<p>The statistical hypotheses are as follow:</p>
<ul>
<li>Null hypothesis (H<sub>0</sub>): the coefficients are equal to zero
(i.e., no relationship between x and y)</li>
<li>Alternative Hypothesis (H<sub>1</sub>): the coefficients are not
equal to zero (i.e., there is some relationship between <span
class="math inline">\(x\)</span> and <span
class="math inline">\(y\)</span>)</li>
</ul>
<p>Mathematically, for a given beta coefficient (<span
class="math inline">\(\beta\)</span>), the <em>t</em>-test is computed
as <span class="math inline">\(t = (\beta - 0)/SE(\beta)\)</span>, where
<span class="math inline">\(SE(\beta)\)</span> is the SE of the
coefficient <span class="math inline">\(\beta\)</span>. Simply said, the
t-statistic measures the number of standard deviations that <span
class="math inline">\(\beta\)</span> is away from 0. Thus a large
<em>t</em>-statistic will produce a small p-value (=different).</p>
<p>The higher the <em>t</em>-statistic (and the lower the p-value), the
more significant the predictor. The symbols to the right (***) visually
specifies the level of significance. The line below the table shows the
definition of these symbols; one star means 0.01 &lt; p &lt; 0.05. The
more the stars beside the variable’s p-value, the more significant the
variable.</p>
<p>A statistically significant coefficient indicates that there is an
association between the predictor (<span
class="math inline">\(x\)</span>) and the outcome (<span
class="math inline">\(y\)</span>) variable.</p>
<p>The <em>t</em>-statistic is a very useful guide for whether or not to
include a predictor in a model. High t-statistics (which go with low
p-values near 0) indicate that a predictor should be retained in a
model, while very low <em>t</em>-statistics indicate a predictor could
be dropped (Bruce and Bruce 2017).</p>
<p><strong>2 - Standard errors and confidence intervals</strong></p>
<p>The standard error measures the variability/accuracy of the beta
coefficients. It can be used to compute the confidence intervals of the
coefficients.</p>
<p>For example, the 95% confidence interval for the coefficient <span
class="math inline">\(\beta1\)</span> is defined as <span
class="math inline">\(\beta1 +/- 2*SE(\beta1)\)</span>, where:</p>
<p>the lower limits of <span class="math inline">\(\beta_1 = \beta_1 -
2*SE(\beta_1) = 0.042414 - 2*(0.005593) = 0.031228\)</span></p>
<p>the upper limits of <span class="math inline">\(\beta1 = \beta1 +
2*SE(\beta1) = 0.042414 + 2*(0.005593) = 0.053600\)</span></p>
<p>That is, there is approximately a 95% chance that the interval
[0.031, 0.053] will contain the true value of <span
class="math inline">\(\beta1\)</span>. Similarly the 95% confidence
interval for <span class="math inline">\(\beta0\)</span> can be computed
as <span class="math inline">\(\beta1 +/- 2*SE(\beta0)\)</span>.</p>
<p>To get this information, either you calculate by hands or you simply
call:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">confint</span>(mod1_plant)</span></code></pre></div>
<pre><code>##                   2.5 %      97.5 %
## (Intercept) -0.43047074 -0.02085828
## temp         0.03137508  0.05345215</code></pre>
</div>
<div id="model-accuracy" class="section level4">
<h4>Model accuracy</h4>
<p>Once you identified that, at least, one predictor variable is
significantly associated the outcome, you should continue the diagnostic
by checking how well the model fits the data. This process is also
referred to as the <em>goodness-of-fit</em></p>
<p>The overall quality of the linear regression fit can be assessed
using the following three parameters, displayed in the model
summary:</p>
<ol style="list-style-type: decimal">
<li><strong>The Residual Standard Error (RSE)</strong></li>
</ol>
<p>The <strong>RSE</strong> (also known as the model sigma) is the
<strong>residual variation</strong>, representing the average variation
of the observations points around the fitted regression line. This is
the <strong>standard deviation of residual errors</strong>.</p>
<p>RSE provides an absolute measure of patterns in the <strong>data that
can’t be explained by the model</strong>. When comparing two models, the
model with the small RSE is a good indication that this model fits the
best the data.</p>
<p>Dividing the RSE by the average value of the outcome variable will
give you the prediction error rate, which should be as small as
possible.</p>
<p>In our example, RSE = <code>0.6848</code>, meaning that the observed
Petal.width values deviate from the true regression line by
approximately <code>0.6848</code> units in average.</p>
<p>Whether or not an RSE of <code>0.6848</code> units is an acceptable
prediction error is subjective and depends on the problem context.</p>
<p>However, we can calculate the percentage error. In our data set, the
mean value of <code>loght</code> is 0.458267, and so the percentage
error is 0.6848/0.458267 * 100 = 149 %. There is indeed a high
variation.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">sigma</span>(mod1_plant)<span class="sc">*</span><span class="dv">100</span><span class="sc">/</span><span class="fu">mean</span>(Plant_height<span class="sc">$</span>loght)</span></code></pre></div>
<pre><code>## [1] 149.4371</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><strong>The R-squared (<span
class="math inline">\(R^2\)</span>)</strong></li>
</ol>
<p>The <strong>R-squared</strong> <span
class="math inline">\(R^2\)</span> ranges from 0 to 1 and represents the
proportion of information (i.e. variance) in the response that is
explained by the explanatory variable(s). The <strong>adjusted <span
class="math inline">\(R^2\)</span></strong> adjusts <span
class="math inline">\(R^2\)</span> with the <strong>degrees of
freedom</strong>.</p>
<p>The <span class="math inline">\(R^2\)</span> measures, how well the
model fits the data. For a simple linear regression, <span
class="math inline">\(R^2\)</span> is the square of the <strong>Pearson
correlation coefficient</strong>.</p>
<p>A high value of <span class="math inline">\(R^2\)</span> is a good
indication. However, as the value of <span
class="math inline">\(R^2\)</span> tends to increase when more
predictors are added in the model, such as in <strong>multiple linear
regression mode</strong>l, you should mainly consider the adjusted <span
class="math inline">\(R^2\)</span>**, which is a penalized <span
class="math inline">\(R^2\)</span> for a higher number of
predictors.</p>
<ul>
<li><p>An (adjusted) <span class="math inline">\(R^2\)</span> that is
close to 1 indicates that a large proportion of the variability in the
outcome has been explained by the regression model.</p></li>
<li><p>A number near 0 indicates that the regression model did not
explain much of the variability in the outcome.</p></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><strong>F-statistic</strong></li>
</ol>
<p>The <strong>F-statistic</strong> gives the overall significance of
the model. It assess whether <strong>at least one predictor variable has
a non-zero coefficient</strong>.</p>
<p>In a simple linear regression, this test is not really interesting
since it just duplicates the information in given by the
<em>t</em>-test, available in the coefficient table. In fact, the
<em>F</em>-test is identical to the square of the t-test: <span
class="math inline">\(57.5 = (7.583)^2\)</span>. That would be true in
any model with 1 degree of freedom.</p>
<p>The F-statistic becomes <strong>more important</strong> once we start
<strong>using multiple predictors</strong> as in multiple linear
regression.</p>
<p>A large F-statistic will corresponds to a statistically significant
<em>p</em>-value (p &lt; 0.05). In our example, the F-statistic equal
57.5 producing a p-value of &lt; 1.868e-12, which is highly
significant.</p>
</div>
<div id="other-useful-functions" class="section level4">
<h4>Other useful functions</h4>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">fitted</span>(fit1) <span class="co"># predicted values</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="fu">residuals</span>(fit1) <span class="co"># residuals</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="fu">anova</span>(fit1) <span class="co"># anova table</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="fu">vcov</span>(fit1) <span class="co"># covariance matrix for model parameters</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="fu">influence</span>(fit1) <span class="co"># regression diagnostics</span></span></code></pre></div>
</div>
</div>
<div id="model-assumption" class="section level3">
<h3>Model assumption</h3>
<p><strong>Linearity</strong> There is no point trying to fit a staight
line to data that are curved! Curvilinear relationships produce U-shaped
patterns in plots of the residuals vs the fitted values. Using the plot
function on a model object provides a series of four graphical model
diagnostics, the first of which is a plot of residuals versus fitted
values.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">plot</span>(mod1_plant, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The absence of strong pattern in the above plot indicates the
assumption of linearity is valid. If there is strong pattern, one
potential solution is to log-transform the response. Note in the above
example plant height had already been log-transformed.</p>
<p>Click <a href="https://gallery.shinyapps.io/slr_diag/">here</a> to
see a nice interactive app that shows you what patterns of residuals you
would expect with curved relationships</p>
<p><strong>Constant variance</strong> An even spread of data around the
regression line is desirable. If the plot of residuals versus fitted
values is fan-shaped the assumption of constant variance (aka
homogeneity of variance) is violated. A log-transformation of the
response variable may fix this problem, but if it doesn’t the best
solution is to use a different error distribution in a generalised
linear model framework (GLM).</p>
<p><strong>Normality</strong> Checks of whether the data are normally
distributed are usually performed by either plotting a histogram of the
residuals or via a quantile plot where the residuals are plotted against
the values expected from a normal distribution (the second of the
figures obtained by <code>plot(mod_plant)</code>. If the points in the
quantile plot lie mostly on the line, the residuals are normally
distributed. Violations of normality can be fixed via transformations or
by using a different error-distribution in a GLM. Note, however, that
linear regression is reasonably robust against violations of
normality.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)) <span class="co"># This code put two plots in the same window</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="fu">hist</span>(mod1_plant<span class="sc">$</span>residuals) <span class="co"># Histogram of residuals</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="fu">plot</span>(mod1_plant, <span class="at">which =</span> <span class="dv">2</span>) <span class="co"># Quantile plot</span></span></code></pre></div>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><strong>Independence</strong> The observations of the response should
be independent of each other. Non-independent observations are those
that are in some way associated with each other beyond that which is
explained by the predictor variable(s). For instance, samples collected
from the same site, or repeatedly from the same object, may be more
alike due to some additional factor other than the measured explanatory
variable. Ensuring independence is an issue of experimental and sampling
design and we usually know if the data are independent or not in advance
of our analysis.</p>
<p>There are a variety of measures for dealing with non-independence.
These include ensuring all important predictors are in the model;
averaging across nested observations; or using a mixed-model.</p>
<p>Based on Pena and Slate (2006), the four assumptions in linear
regression are normality, heteroscedasticity, and linearity, and what
the authors refer to as uncorrelatedness. The <code>gvlma( )</code>
function in the <code>gvlma</code> package, performs a global validation
of linear model assumptions as well separate evaluations of skewness,
kurtosis, and heteroscedasticity.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>gvmodel <span class="ot">&lt;-</span> <span class="fu">gvlma</span>(mod1_plant)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="fu">summary</span>(gvmodel)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght ~ temp, data = Plant_height)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.97903 -0.42804 -0.00918  0.43200  1.79893 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.225665   0.103776  -2.175    0.031 *  
## temp         0.042414   0.005593   7.583 1.87e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6848 on 176 degrees of freedom
## Multiple R-squared:  0.2463, Adjusted R-squared:  0.242 
## F-statistic:  57.5 on 1 and 176 DF,  p-value: 1.868e-12
## 
## 
## ASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS
## USING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:
## Level of Significance =  0.05 
## 
## Call:
##  gvlma(x = mod1_plant) 
## 
##                      Value p-value                Decision
## Global Stat        0.47538  0.9759 Assumptions acceptable.
## Skewness           0.29268  0.5885 Assumptions acceptable.
## Kurtosis           0.11168  0.7382 Assumptions acceptable.
## Link Function      0.02790  0.8673 Assumptions acceptable.
## Heteroscedasticity 0.04311  0.8355 Assumptions acceptable.</code></pre>
</div>
</div>
<div id="two-or-more-predictors" class="section level2">
<h2>Two (or more) predictors</h2>
<p><strong>Multiple linear regression</strong> is just an
<strong>extension of simple linear regression</strong> used to predict
an outcome variable (<span class="math inline">\(y\)</span>) on the
basis of multiple distinct predictor variables (<span
class="math inline">\(x\)</span>).</p>
<p>With three predictor variables (<span
class="math inline">\(x\)</span>), the prediction of <span
class="math inline">\(y\)</span> is expressed by the following
equation:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1*x_1 + \beta_2*x_2
+ \beta_3*x_3\]</span> The <span class="math inline">\(\beta\)</span>
values measure the association between the predictor variable and the
outcome. “<span class="math inline">\(\beta_j\)</span>” can be
interpreted as the average effect on <span
class="math inline">\(y\)</span> of a one unit increase in <span
class="math inline">\(x_j\)</span>, holding all other predictors
fixed.</p>
<p><span class="math inline">\(loght = \beta_0 + \beta_1 * temp +
\beta_2 * alt + \beta_3 * rain\)</span></p>
<p>or</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>mod2_plant<span class="ot">&lt;-</span> <span class="fu">lm</span>(loght <span class="sc">~</span> temp <span class="sc">+</span> alt <span class="sc">+</span> rain, <span class="at">data =</span> Plant_height)</span></code></pre></div>
<p>A common problem that arises in multiple linear regression is the
<strong>multi-collinearity</strong>. This is the situation when two or
more predictors are highly linearly related between them.
Multicollinearitiy has important effects on the fit of the model:</p>
<ul>
<li><p>It reduces the precision of the estimates. As a consequence,
signs of fitted coefficients may be reversed and valuable predictors may
appear as non significant.</p></li>
<li><p>It is difficult to determine how each of the highly related
predictors affects the response, since one masks the other. This may
result in numerical instabilities.</p></li>
</ul>
<p>An approach is to detect multicollinearity is to compute a
correlation matrix between the predictors as we learned earlier</p>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Here we can see there is a significant correlation between the
predictors. A better approach is to compute the <strong>Variance
Inflation Factor</strong> (VIF) of each coefficient <span
class="math inline">\(\beta_j\)</span>. This is measure of how linearly
dependent is <span class="math inline">\(X_j\)</span> with the rest of
predictors:</p>
<p><span
class="math display">\[\text{VIF}(\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}}\]</span>
where <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span> is the <span
class="math inline">\(R^2\)</span> from a regression of <span
class="math inline">\(X_j\)</span> into the remaining predictors. The
next rule of thumb gives direct insight into which predictors are
multicollinear:</p>
<ul>
<li><strong>VIF close to 1</strong>: absence of multicollinearity.</li>
<li><strong>VIF larger than 5 or 10</strong>: multicolinearity
<strong>problematic</strong>.</li>
</ul>
<p>Others considered <span
class="math inline">\(\sqrt{VIF}&gt;2\)</span> as critical limit to
consider multicollinearity.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Evaluate Collinearity</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(mod2_plant) <span class="co"># variance inflation factors</span></span></code></pre></div>
<pre><code>##     temp      alt     rain 
## 1.590787 1.140424 1.438287</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="fu">sqrt</span>(car<span class="sc">::</span><span class="fu">vif</span>(mod2_plant)) <span class="sc">&gt;</span> <span class="dv">2</span> <span class="co"># problem?</span></span></code></pre></div>
<pre><code>##  temp   alt  rain 
## FALSE FALSE FALSE</code></pre>
<p>None of the predictors seem problematic here.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="fu">summary</span>(mod2_plant)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loght ~ temp + alt + rain, data = Plant_height)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.83849 -0.49760 -0.00025  0.39435  1.59110 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.553e-01  1.264e-01  -2.810 0.005514 ** 
## temp         2.906e-02  6.793e-03   4.278  3.1e-05 ***
## alt          3.701e-05  1.045e-04   0.354 0.723519    
## rain         2.457e-04  6.226e-05   3.947 0.000115 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6595 on 174 degrees of freedom
## Multiple R-squared:  0.309,  Adjusted R-squared:  0.2971 
## F-statistic: 25.93 on 3 and 174 DF,  p-value: 6.461e-14</code></pre>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="fu">plot</span>(mod2_plant)</span></code></pre></div>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="model-selection" class="section level2">
<h2>Model selection</h2>
<p>If you aim is to predict you are looking for the best model. An
information criterion balances the fitness of a model with the number of
predictors employed. Hence, it determines objectively the best model as
the one that minimizes the information criterion. Two common criteria
are the <strong>Bayesian Information Criterion (BIC)</strong> and the
<strong>Akaike Information Criterion (AIC)</strong>.</p>
<p><span class="math inline">\(AIC(model)=-2*logLik(model) + npar(model)
* 2\)</span></p>
<p><span class="math inline">\(BIC(model)=-2*logLik(model) + npar(model)
* log(n)\)</span></p>
<p>where <span class="math inline">\(Lik(model)\)</span> is the
likelihood of the model (how well the model fits the data) and <span
class="math inline">\(npar(model)\)</span> is the number of parameters
of the model, <span class="math inline">\(k+2\)</span> in the case of a
multiple linear regression model with <span
class="math inline">\(k\)</span> predictors.The AIC replaces <span
class="math inline">\(log(n)\)</span> by <span
class="math inline">\(2\)</span>, so it penalizes less complex
models.This is one of the reasons why BIC is preferred by some
practitioners for model comparison. Also, because is consistent in
selecting the true model: if enough data is provided, the BIC is
guaranteed to select the data-generating model among a list of candidate
models.</p>
<p>Both are based on a <strong>balance between the model fitness and its
complexity</strong>.Both BIC and AIC can be computed in <code>R</code>
through the functions <code>BIC</code> and <code>AIC</code>. They take a
model as the input. The lower the better with a rule of thumb = 2.</p>
<p>Note: Do you remember about <code>sigma</code> (the Residual Standard
Error, RSE)? We previously used it to calculate a pecrcentage error.
Well, AIC and BIC used the log likelihood of the model obtain using
<code>logLik (model)</code>. You can recalculate this logLik using:
<code>sum(log(dnorm(x = y, mean = predict(model), sd = sigma(model))))</code>
which illustrate the connection between <code>sigma</code>and
information criterion.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>mod3_plant<span class="ot">&lt;-</span><span class="fu">lm</span>(<span class="at">formula =</span> loght <span class="sc">~</span> temp <span class="sc">+</span> rain, <span class="at">data =</span> Plant_height)</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="fu">BIC</span>(mod1_plant); <span class="fu">BIC</span>(mod2_plant); <span class="fu">BIC</span>(mod3_plant)</span></code></pre></div>
<pre><code>## [1] 383.8952</code></pre>
<pre><code>## [1] 378.7943</code></pre>
<pre><code>## [1] 373.7409</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">AIC</span>(mod1_plant); <span class="fu">AIC</span>(mod2_plant); <span class="fu">AIC</span>(mod3_plant)</span></code></pre></div>
<pre><code>## [1] 374.3499</code></pre>
<pre><code>## [1] 362.8854</code></pre>
<pre><code>## [1] 361.0138</code></pre>
<p>However, selecting a subset of predictor variables from a larger set
(e.g., stepwise selection) remains a controversial topic. You can
perform stepwise selection (forward, backward, both) using the
<code>stepAIC()</code> and <code>stepBIC()</code> function from the
<code>MASS</code> package. <code>stepAIC()</code> performs stepwise
model selection by exact AIC.</p>
</div>
</div>
<div id="anova" class="section level1">
<h1>ANOVA</h1>
<p>Analysis of variance (ANOVA) is one of the most frequently used
techniques in the biological and environmental sciences. ANOVA is used
to contrast a continuous dependent variable y across levels of one or
more categorical independent variables x. The independent variables are
termed the factor or treatment, and the various categories within that
treatment are termed the levels. In this module, we will start with the
simplest design - those with a single factor.</p>
<p>Where an independent samples t-test would be used for comparison of
group means across two levels, ANOVA is used for the comparison of &gt;2
group means, or when there are more than two or more predictor
variables. The logic of this test is essentially the same as the t-test
- it compares variation between groups to variation within groups to
determine whether the observed differences are due to chance or not.</p>
<div id="one-way-anova" class="section level2">
<h2>One-way ANOVA</h2>
<p>Also called single factor ANOVA.</p>
<p>For example, to contrast the the hatching times of turtle eggs
incubated at four different temperatures (15°C, 20°C, 25°C and 30°C),
hatching time is the continuous response variable and temperature is the
categorical predictor variable with with four levels. The null
hypothesis would be that mean hatching time is equal for all
temperatures.</p>
<p><span
class="math inline">\(H_0=\mu_{15}=\mu_{20}=\mu_{25}=\mu_{30}\)</span></p>
<p>Note that an <strong>ANOVA is a linear model</strong>, just like
linear regression except that the predictor variables are categorical
rather than continuous.</p>
<p><span class="math inline">\(y_{ij}=\mu + \alpha_i +
\epsilon_{ij}\)</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the overall mean
and <span class="math inline">\(\alpha_i\)</span> is the effect of the
<span class="math inline">\(i^{th}\)</span> group.</p>
<p>It is the same as a multiple linear regression with a predictor
variable for each level of the categorical variable (each coded as a
dummy variable). For the question of whether hatching time of turtles
differs between four incubation tempeatures, we must fit four parameters
to describe the mean response of each temperature (rather than just a
single intercept and single slope in a simple linear regression). For
this example, our linear model equation will have this form:</p>
<p>$HatchingTime = + <em>1.Temp</em>{15} + <em>1.Temp</em>{20} +
<em>1.Temp</em>{25} + <em>1.Temp</em>{30} + $</p>
<p>ANOVA partitions the total variance into a component that can be
explained by the predictor variable (among levels of the treatment), and
a component that cannot be explained (within levels, the residual
variance). The test statistic, F, is the ratio of these two sources of
variation.</p>
<p><span class="math inline">\(F=MS_{among}/MS_{within}\)</span></p>
<p>where MS are the mean squares, a measure of variation. The
probability of obtaining the observed value of F is calculated from the
known probability distribution of F, with two degrees of freedom (one
for the numerator = the number of levels -1) and one for the denominator
(number of replicates per level - 1 x number of levels).</p>
<div id="running-the-analysis-1" class="section level3">
<h3>Running the analysis</h3>
<p>The data should be formatted such that the individual replicates are
rows and the variables are separate columns. Include a column for the
dependent variable, <span class="math inline">\(y\)</span>, and a
corresponding column for the categorical variable, <span
class="math inline">\(x\)</span>. Download the sample data set for the
turtle hatching example, <code>turtles.csv</code>, import into R and
check that temperature variable is a factor with the <code>str</code>
function.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a>turtles <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="at">file =</span> <span class="st">&quot;data/turtles.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="fu">str</span>(turtles)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  2 variables:
##  $ Temperature: int  15 15 15 15 15 15 15 15 15 15 ...
##  $ Days       : int  37 43 45 54 56 65 62 73 74 75 ...</code></pre>
<p>In this case, because we have numbers for the four levels of the
Temperature treatment, we need to change that variable to become a
factor rather than an integer.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a>turtles<span class="sc">$</span>Temperature <span class="ot">&lt;-</span> <span class="fu">factor</span>(turtles<span class="sc">$</span>Temperature)</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="fu">boxplot</span>(Days <span class="sc">~</span> Temperature, <span class="at">data =</span> turtles, <span class="at">ylab =</span> <span class="st">&quot;Hatching time (days)&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Temperature (°C)&quot;</span>)</span></code></pre></div>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Now, we can run the analysis of variance contrasting hatching time
(days) across temperatures using the function <code>aov</code>. The
arguments of the function are simply a formula statement,
<code>y~x</code>, with the response variable to the left of the
<code>~</code>, the predictor variable to the right, and some code to
specify which data frame holds those variables.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>turtles.aov <span class="ot">&lt;-</span> <span class="fu">aov</span>(Days <span class="sc">~</span> Temperature, <span class="at">data =</span> turtles)</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="fu">summary</span>(turtles.aov)</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Temperature  3   8025  2675.2   15.98 9.08e-07 ***
## Residuals   36   6027   167.4                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Check but the same can be produced by passing a <code>lm</code> in
<code>anova()</code>:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a>turtles.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(Days <span class="sc">~</span> Temperature, <span class="at">data =</span> turtles)</span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a><span class="fu">anova</span>(turtles.lm) </span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Days
##             Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## Temperature  3 8025.5 2675.16  15.978 9.082e-07 ***
## Residuals   36 6027.3  167.42                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The summary output of an ANOVA object is a table with the degrees of
freedom (Df), sums of squares (Sum Sq), mean squares (Mean Sq) for the
predictor variable (i.e., variation among levels of your treatment) and
for the Residuals (i.e., varation within the levels). The test
statistic, <span class="math inline">\(F\)</span> value and its
associated p-value (Pr(&gt;F)) are also presented.</p>
<p>First check the degrees of freedom. The factor Df = the number of
levels of your factor - 1. The residual <span class="math inline">\(Df =
a(n-1)\)</span>, where <span class="math inline">\(a\)</span> = the
number of levels of your factor and <span
class="math inline">\(n\)</span> = sample size (replicates per
level).</p>
<p>The sums of squares and mean squares are measures of variation. The
<span class="math inline">\(F\)</span> statistic is the ratio of <span
class="math inline">\(MS_{among}\)</span> and <span
class="math inline">\(MS_{within}\)</span> and the p-value is the
probability of the observed <span class="math inline">\(F\)</span> value
from the <span class="math inline">\(F\)</span> distribution (with the
given degrees of freedom).</p>
<p>The main thing to look at in the ANOVA table is whether your
predictor variable had a significant effect on your response variable.
In this example, the probability that all four incubation temperatures
are equal is &lt;0.001. This is very unlikely and much less than 0.05.
We would conclude that there is a difference in hatching times between
the temperatures.</p>
<p>In the <code>lm</code> output , you get a bit more information.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="fu">summary</span>(turtles.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Days ~ Temperature, data = turtles)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -28.200  -9.225   1.650   9.025  19.400 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     58.400      4.092  14.273  &lt; 2e-16 ***
## Temperature20  -13.800      5.787  -2.385   0.0225 *  
## Temperature25   -9.200      5.787  -1.590   0.1206    
## Temperature30  -38.300      5.787  -6.619 1.04e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.94 on 36 degrees of freedom
## Multiple R-squared:  0.5711, Adjusted R-squared:  0.5354 
## F-statistic: 15.98 on 3 and 36 DF,  p-value: 9.082e-07</code></pre>
<p>The output for the standard ANOVA table is down the bottom and above
it you get the actual parameter estimates from the linear model (the
<span class="math inline">\(\beta_1\)</span>, <span
class="math inline">\(\beta_2\)</span>, etc. from above). In this
example, turtles at 15°C hatched after 58.4 days, on average (the
intercept in the model). The other parameter estimates are differences
between each level of temperature and the intercept. For example, at
20°C they were 13.8 days faster (i.e., the mean for 20°C = 58.4-13.8 =
44.6 days).</p>
<p>If you detect any significant differences in the ANOVA, we are then
interested in knowing exactly which groups differ from one another, and
which do not. Remember that a significant p value in the test you just
ran would reject the null hypothesis the means of the dependent variable
were the same across all groups, but not identify which were different
from each other. To see a comparison between each mean and each other
mean, we can use a Tukey’s post-hoc test.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="fu">TukeyHSD</span>(turtles.aov)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Days ~ Temperature, data = turtles)
## 
## $Temperature
##        diff       lwr        upr     p adj
## 20-15 -13.8 -29.38469   1.784689 0.0982694
## 25-15  -9.2 -24.78469   6.384689 0.3969971
## 30-15 -38.3 -53.88469 -22.715311 0.0000006
## 25-20   4.6 -10.98469  20.184689 0.8562615
## 30-20 -24.5 -40.08469  -8.915311 0.0008384
## 30-25 -29.1 -44.68469 -13.515311 0.0000785</code></pre>
</div>
<div id="assumptions" class="section level3">
<h3>Assumptions</h3>
<p>As for any linear models, The important assumptions of ANOVA are
independence, homogeneity of variance and normality. We advocate a
qualitative evalutation of the normality and homogeneity of variance
assumptions, by examining the patterns of variation in the residuals,
rather than a formal test. Linear models in general are quite ‘robust’
for violating these assumptions (heterogeneity and normality), within
reason.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)) <span class="co"># This code put two plots in the same window</span></span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="fu">hist</span>(turtles.aov<span class="sc">$</span>residuals)</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a><span class="fu">plot</span>(turtles.aov, <span class="at">which =</span> <span class="dv">2</span>)</span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="fu">plot</span>(turtles.aov, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="stat_2B_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Violations of normality and homogeneity of variance can be fixed via
transformations or by using a different error-distribution in a
generalised linear model (GLM).</p>
</div>
</div>
<div id="factorial-anova" class="section level2">
<h2>Factorial ANOVA</h2>
<p>Consider an example where a researcher is testing the effects of
metal contamination on the number of species found in sessile marine
invertebrates (sponges, bryozoans and sea squirts etc.). They would like
to know whether copper reduces species richness, but also know that the
richness of invertebrates can depend on whether the substrate is
vertical or horizontal. Consequently, they ran an experiment where
species richness was recorded in replicate samples in each of the six
combinations of copper enrichment (“None”,“Low”,“High”) and orientation
(“Vertical”,“Horizontal”). The experimental design in termed
<strong>factorial</strong> because <strong>all levels of one treatment
are represented in all levels of the other treatment (also termed
orthogonal)</strong>.</p>
<p>The factorial ANOVA will test:</p>
<ul>
<li>whether there are any differences in richness among the three levels
of copper enrichment</li>
<li>whether there are any differences in richness among the two levels
of substrate orientation</li>
<li>whether there is any interaction between copper and orientation</li>
</ul>
<p>You have three null hypotheses:</p>
<ul>
<li><p>there is no difference between the means for each level of
copper, Ho: <span
class="math inline">\(\mu_{none}=\mu_{low}=\mu_{high}\)</span></p></li>
<li><p>there is no difference between the means for each level of
orientation, Ho: <span
class="math inline">\(\mu_{vertical}=\mu_{horizontal}\)</span></p></li>
<li><p>there is no interaction between the factors</p></li>
</ul>
<p>This is far better than running two separate single factor ANOVAs
that contrast copper effects for each level of orientation because you
have more statistical power (higher degrees of freedom) for the tests of
interest, and you get a formal test of the interaction between factors
which is often scientifically interesting.</p>
<p><span class="math inline">\(y_{ij}=\mu + \alpha_i + \beta_j +
(\alpha\beta)_{ij} + \epsilon_{ijk}\)</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the overall mean,
<span class="math inline">\(\alpha_i\)</span> is the effect of the
i<sup>th</sup> group of the first factor</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
